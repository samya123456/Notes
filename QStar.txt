Reinforcement learning pipeline
Rejection sampling 
Outcome-supervised reward models (ORMs)
process-supervised reward models
(PRMs) receive feedback for each step in the chain-of-thought

Within the domain of logical reasoning, models trained with outcome
supervision regularly use incorrect reasoning to reach the correct final answer
(Zelikman et al., 2022; Creswell et al., 2022).

We conduct our own detailed comparison of outcome and
process supervision, with three main differences: we use a more capable base
model, we use significantly more human feedback, and we train and test on the
more challenging MATH dataset (Hendrycks et al., 2021).
Our main contributions are as follows:
1. We show that process supervision can train much more reliable reward
models than outcome supervision. We use our state-of-the-art PRM to
solve 78.2% of problems from a representative subset of the MATH test
set.
2. We show that a large reward model can reliably approximate human supervision for smaller reward models, and that it can be used to efficiently
conduct large-scale data collection ablations.
3. We show that active learning leads to a 2.6Ã— improvement in the data
efficiency of process supervision.
4. We release our full process supervision dataset, PRM800K, to promote
related research.

At each model scale, we use a single fixed model to generate all solutions. We
call this model the generator

***We instead focus exclusively on how to train the most reliable reward model
possible. We evaluate a reward model by its ability to perform best-of-N search
over uniformly sampled solutions from the generator. For each test problem we
select the solution ranked highest by the reward model, automatically grade it
based on its final answer, and report the fraction that are correct. A reward
model that is more reliable will select the correct solution more often.
