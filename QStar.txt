Reinforcement learning pipeline
Rejection sampling 
Outcome-supervised reward models (ORMs)
process-supervised reward models
(PRMs) receive feedback for each step in the chain-of-thought

Within the domain of logical reasoning, models trained with outcome
supervision regularly use incorrect reasoning to reach the correct final answer
(Zelikman et al., 2022; Creswell et al., 2022).

We conduct our own detailed comparison of outcome and
process supervision, with three main differences: we use a more capable base
model, we use significantly more human feedback, and we train and test on the
more challenging MATH dataset (Hendrycks et al., 2021).
Our main contributions are as follows:
1. We show that process supervision can train much more reliable reward
models than outcome supervision. We use our state-of-the-art PRM to
solve 78.2% of problems from a representative subset of the MATH test
set.
2. We show that a large reward model can reliably approximate human supervision for smaller reward models, and that it can be used to efficiently
conduct large-scale data collection ablations.
3. We show that active learning leads to a 2.6Ã— improvement in the data
efficiency of process supervision.
4. We release our full process supervision dataset, PRM800K, to promote
related research.

At each model scale, we use a single fixed model to generate all solutions. We
call this model the generator

***We instead focus exclusively on how to train the most reliable reward model
possible. We evaluate a reward model by its ability to perform best-of-N search
over uniformly sampled solutions from the generator. For each test problem we
select the solution ranked highest by the reward model, automatically grade it
based on its final answer, and report the fraction that are correct. A reward
model that is more reliable will select the correct solution more often.

math-relevant tokens, which we call MathMix.

2.3 Generator
To make parsing individual steps easier, we train the generator to produce
solutions in a newline delimited step-by-step format. Specifically, we few-shot
generate solutions to MATH training problems, filter to those that reach the
correct final answer, and finetune the base model on this dataset for a single
epoch. This step is not intended to teach the generator new skills; it is intended
only to teach the generator to produce solutions in the desired format.

2.4 Data Collection
To collect process supervision data, we present human data-labelers with stepby-step solutions to MATH problems sampled by the large-scale generator.
Their task is to assign each step in the solution a label of positive, negative,
or neutral, as shown in Figure 1. A positive label indicates that the step is correct and reasonable. A negative label indicates that the step is either incorrect
or unreasonable. A neutral label indicates ambiguity. In practice, a step may
be labelled neutral if it is subtly misleading, or if it is a poor suggestion that
is technically still valid. We permit neutral labels since this allows us to defer
the decision about how to handle ambiguity: at test time, we can treat neutral
labels as either positive or negative

We refer to the entire dataset of step-level labels collected as PRM800K. The PRM800K training
set contains 800K step-level labels across 75K solutions to 12K problems.

3 --------------
