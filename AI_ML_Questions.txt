1) Machine Learning: Automated Analytical model
2) Neural Network: A Type of machine learning Architecture modeled after biological neurons.
3) Deep Learning: A Neural Network with more than one hidden layer.

4) Overfitting:
when your Model touches every single point of your training data, your error is really low in the training set. but it doesn't create a good prediction model as it doesn't fall under any mathematical equation y = f(x). that's why we get an error large in the validation set or in the test set. this is called Overfitting i.e. fitting too much to the noise data.


Overcome Overfitting
Early stopping. Early stopping pauses the training phase before the machine learning model learns the noise in the data.
Pruning. You might identify several features or parameters that impact the final prediction when you build a model. ...

5) Good Model should look like having large errors at the beginning and errors will reduce over training a period of time.
6) Epochs: 1 epoch is running the entire training data 1 time through your model.

Model Evaluation Matrix: Classification Problem
================================================
  i) Accuracy = number of correct predictions / total number of predictions.
  ii) Recall = number of true positive / (number of true positive + number of false negative)
  iii) Precision = number of true positive / (number of true positive + number of false positive)
  iv) f1 Score = combination of Recall and Precision  matrix = 2*(Recall * Precision) / (Recall + Precision)

True positive: the pic is of the Dog and the model predicted it as Dog.
True Negative: the pic is not of the Dog and the model predicted it's not a Dog.
False positive: the pic is not of the Dog and the model predicted it's a Dog.
False Negative: the pic is of the Dog and the model predicted it's not a Dog.

Confusion Matrix:  We can Organise our predicted values compared to the real values in the Cofusion matrix

Model Evaluation Matrix: Regression Problem
===============================================
A linear Regression Problem is when your model is continuously predicting the result of a problem

 i) Mean Square Error
 ii) Mean Square Error
 iii) Root Mean Square Error


Unsupervised Learning
====================
Dealing with unlabeled data.

Tasks in unsupervised Learning:
==============================
i) Clustering - Grouping together unlabeled data points into clusters based on similarity
ii) Anomaly Detection - Attempt to detect outliers (Exceptions) in the dataset
iii) dimensionality reduction


Difference between supervised learning and Unsupervised learning stages: Only difference is that we don't split the dataset into Train and Test datasets in Unsupervised learning as the data is unlabeled. 


Percepron Model
===============


Single Biological Neuron ---> Percepron ---> Multi-Layer Perceptron Model ---> Deep learning Neural network

y = f(x) = x1*w1 + x2*w2 ; but if x1 or x2 =0 then that input will not be having any impact in the output. To fix that we add b.

y = f(x) = (x1*w1 + b) + (x2*w2 + b)

Neural network
==============

1st layer => input layer
last layer => output layer
middle layers => hidden layer


Activation Functions
====================

y = f(x) = x*w + b 

w = weight 
b = bias

if you want to set a boundary to the output 

z = x*w + b 

and we pass z in some activation function to limit the value

if we are doing a binary classification problem the output will be 0 or 1.
When we are doing Binary classification problems there are the Activation function functions used: 
https://ibm-learning.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/learn/lecture/16844498#overview

	 A) Sigmoid function: 
	 ====================
	 
	 f(z) = 1/ (1 + e^(-z))
	 the output value will be 0 to 1
	 
	 B) Hyperbolic Tangent: 
	 ======================
	 
	 f(z) = tanh(z)
	 the output value will be -1 to 1
	 
	 C) Rectified linear unit: (most used Activation funcion)
	 =========================
	 f(z) = max(0 , z) , if value is > 0 then 0 else z
 
 
When we are doing Multi-class  classification problems there are the Activation function functions used: 
https://ibm-learning.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/learn/lecture/16844504#overview

 ** in multi-class classification problem we are 2 situations
 i) Non-Exclusive classes: A data point can have multiple classes/categories assigned to it.
   Ex - photos can have multiple tags, beach, family vacations, etc.
   
   
 ii) Mutually Exclusive classes:
   Ex - Photos in grayscale can be categorized as black n white or full color
   
 
To organize data for  Multi-class  classification problems  we use one-hot encoding

 Non Exclusive Multi-class  classification problems Activation function :
	 A) Sigmoid function: 
	 ====================
	 
	 f(z) = 1/ (1 + e^(-z))
	 the output value will be 0 to 1
 Mutually Exclusive classes classification problems Activation function :
 
     A) SoftMax activation function:
	 ==============================
	 This function will calculate the probability of each target class, over all possible target classes.	
	 this probability will range between 0 to 1.
	 
	 this model returns the probabilities of each class and the target class will be chosen will have the highest probability.
	 
	 Example :
	  Red = 0.1
	  Green = 0.6
	  Blue = 0.3
	  
	  probablity sum =1
	  
	  so in this case we'll choose the target class as green as it has the highest probability
	  
	

Cost Function:
==============	
https://ibm-learning.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/learn/lecture/16844516#overview



*Cost Function tells you how far you are from the true value based on your prediction.

*It Compares Our Neural Network with the true Value we use the cost function or loss function or error function.

 Here :
 =====
 y = true value 
 a = neural predictions
 
 Commonly used Cost function:
 ============================
 
 A) Quadratic Cost Function: Which essentially means root mean square error
 
 The cost function consists of four main things :
 W = weight
 B = Bias
 Sr = input of a single training sample
 Er = desired output










INTERVIEW QUESTIONS:
1) what is embedding
2) How chatGPT supports local languages although there is so low data set to train a model. --> First, ChatGPT uses its language-processing features to identify the language you’ve used in your question/prompt. It uses natural language processing (NLP) techniques to understand the intent and emotion behind the text.

optimization
rule based algo -- mean square error , root mean square error
pysprak ***
databricks
=================
deploy model
issues faced different with data , when the data looks good but they are not really that good




Model optimization:
===================

Model optimization in TensorFlow involves a set of techniques aimed at improving the performance, efficiency, and generalization of your machine learning models. There are various approaches to optimizing TensorFlow models

1) Data Preprocessing: Ensuring that your input data is appropriately prepared can have a significant impact on model performance. Techniques like
    i) normalization
    ii) feature scaling
    iii) handling missing values are crucial steps to consider.

2) Choosing the Right Architecture: Choosing the right model is important.
3) Transfer Learning: For certain tasks, using pre-trained models and fine-tuning them on your specific dataset can be more efficient than training from scratch.
4) Pruning: Pruning involves removing insignificant weights from the model, leading to a more compact model with potential speed-up during inference.


Issues faced with data when the data looks good but they are not really that good :
=================================================================================
If your data appears to be of good quality, but your model's performance is not as good as expected, there could be several reasons for this discrepancy. Here are some steps to investigate and improve the situation:

	i) Data Exploration and Visualization: Revisit your data exploration process to ensure you haven't missed any important patterns or insights. Visualize the data in different ways to get a better understanding of its characteristics and potential issues.

	ii) Data Preprocessing: Check if your data preprocessing steps are appropriate. Ensure that you are
		1) handling missing values, 
                2) scaling features, and applying any necessary transformations correctly. 
	Incorrect preprocessing can lead to suboptimal model performance.

	iii) Feature Engineering: Evaluate if you have captured all the relevant features from your data. Sometimes, domain knowledge and creative feature engineering can significantly improve model performance.


Rule-Based Algorithms in Machine Learning:
==========================================

Rule-based algorithms are a class of machine learning approaches that rely on predefined rules or logical conditions to make predictions or decisions. Unlike traditional machine learning algorithms that learn patterns from data, rule-based algorithms are based on explicit rules set by domain experts or through manual analysis. These rules are often represented in the form of "if-then" statements or logical expressions.


Dealing with Missing Data:
=========================

determine the number of missing data in each column.

	i) 	df.isnull().sum()
	ii) determine how much percentage of that particular column has missing data
		100* (df.isnull().sum())/ len(df)
	iii) drop the columns that have very less percentage
	iv) columns that we can't drop, we have to fill data. 
		two approaches:
		1) K Nearest Neighbour
		2) Find out other columns which have the highest correlation with this column and fill out the missing data with those values.



What is Databricks:
==================

The core of data bricks is an open-source distributed computing processing engine called Apache Spark. Microsoft makes the Databricks service available on its Azure cloud platform. These are combined and called Azure Databricks.

Apache Spark is a lighting fast unified analytics engine for big data processing and Machine learning.

MLOPS:
=======

1) Export a model:
================
https://www.youtube.com/watch?v=NVY0FucNRU4
  i) Save the whole model (in HDF5 format)
     model.save("nn.h5")

2) Import a model from the file:
============================
https://www.youtube.com/watch?v=NVY0FucNRU4

  i) model = keras.models.load_model("nn.h5")




Different Machine Learning Models:


1) Linear Regression:
=====================
Use for: Regression problems where the relationship between features and target is approximately linear.
Strengths: Simplicity, interpretability, good for basic regression tasks.
Weaknesses: Assumes a linear relationship, and may not handle complex data well.


2) Logistic Regression:
=======================

Use for: Binary classification problems.
Strengths: Interpretable, works well when the relationship between features and target is not too complex.
Weaknesses: Assumes linear separation boundaries.


3) Decision Trees:
==================

Use for: Both classification and regression problems, especially when dealing with non-linear relationships.
Strengths: Can handle both categorical and numerical features, interpretable (for simpler trees).
Weaknesses: Prone to overfitting, can be sensitive to small changes in data.


4) Random Forests:
==================
Use for: Classification and regression tasks where higher accuracy is desired, and overfitting is a concern.
Strengths: Robust, handles non-linearity, overcomes decision tree limitations.
Weaknesses: Less interpretable than individual decision trees, more complex.


5) Support Vector Machines (SVM):
=================================
Use for: Binary classification tasks, especially when data is not linearly separable.
Strengths: Effective in high-dimensional spaces, can handle non-linear data with kernel trick.
Weaknesses: Computationally expensive for large datasets, may not perform well on noisy data.


6) K-Nearest Neighbors (KNN):
=============================
Use for: Classification and regression tasks, especially when local patterns are important.
Strengths: Simple, non-parametric, can capture complex relationships.
Weaknesses: Can be computationally expensive for large datasets, and sensitive to irrelevant features.

7) Clustering Algorithms (e.g., K-Means, DBSCAN):
==================================================
Use for: Grouping similar data points together (unsupervised learning).
Strengths: Useful for data exploration, finding patterns, and segmenting data.
Weaknesses: Need to specify the number of clusters and sensitivity to initialization.


8) Neural Networks (Deep Learning):
===================================
Use for: Complex tasks like image and speech recognition, natural language processing, and large datasets.
Strengths: Can capture intricate patterns, high accuracy potential.
Weaknesses: Requires large amounts of data, computational resources, and careful tuning.


9) Naive Bayes:
==============
Use for: Text classification, spam filtering, and other probabilistic classification tasks.
Strengths: Simple, works well with high-dimensional data (e.g., text).
Weaknesses: Assumes features are independent, and may not work well with complex data.



Questions:
=========
https://www.youtube.com/watch?v=E5wVPKJkwCg&list=PLKnIA16_Rmva_ZdY31wpEICG0KHnvIhu2&index=4


1) What is the difference between Parametric & Non-Parametric ML Algorithms?

Parametric Algorithms:
======================
Parametric algorithms make strong assumptions about the functional form of the underlying data distribution (y = f(x)). They assume that the data is generated from a specific mathematical model with a fixed number of parameters. Once these parameters are learned from the training data, the model is fully specified and can make predictions.

Examples of parametric algorithms include linear regression, logistic regression, and Naive Bayes classifiers.


Non-Parametric Algorithms:
==========================
Non-parametric algorithms, on the other hand, make fewer assumptions about the underlying data distribution. They don't assume a fixed form of the model and instead, attempt to learn the structure of the data directly from the training examples.

Examples of non-parametric algorithms include k-nearest neighbors, decision trees, support vector machines with non-linear kernels, and neural networks.


2) Difference between convex & non-convex cost function; what happens when the cost function is non-convex?

Convex Cost Function:
====================

A convex cost function is a function where any line segment connecting two points on the curve lies above the curve itself. In other words, it's a function that has a "bowl" shape, and there are no local minima other than the global minimum. Convex cost functions are desirable in optimization because they guarantee that optimization algorithms will converge to the global minimum, which represents the best set of parameters for the model.

Non-Convex Cost Function:
=========================

A non-convex cost function is one that has multiple local minima and may have complex shapes, such as peaks, valleys, and plateaus. This means that there are multiple solutions that the optimization algorithm can converge to, and it might not find the global optimum. Non-convex functions are more challenging to optimize because the optimization algorithms can get stuck in a local minimum instead of finding the global minimum.

What Happens When Cost Function Is Non-Convex:

When the cost function is non-convex, the optimization process becomes more challenging and uncertain. The model might fail to reach the best possible performance, and its training could be more sensitive to hyperparameters and initial conditions.

3) Why is 'Naïve' Bayes naïve?


Naïve Bayes is a machine learning algorithm based on Bayesian probability theory that is used for classification and probabilistic modeling.

Naïve Bayes assumes that features (attributes or variables) are independent of each other, given the class label. This means that the presence or absence of one feature does not affect the presence or absence of another feature

4) What do you mean by the unreasonable effectiveness of data?

The phrase “unreasonable effectiveness of data” in machine learning refers to the remarkable ability of large datasets to produce highly accurate models, even when the algorithms used to analyze them are relatively simple.

5) What is Lazy learning

In lazy learning, the model stores the entire training dataset and uses it directly to make predictions on new, unseen data instances.

Example: KNN

6) What is Eger learning 
	
This is in contrast to eager learning (or model-based learning), where a model is trained on the entire dataset before making predictions.

Example: Most of the Machine learning model falls under Eger Learning.


7) What do you mean by semi-supervised learning?


Semi-supervised learning involves a small portion of labeled examples and a large number of unlabeled examples from which a model must learn and make predictions on new examples.


8) What is an OOB(out of the bag) error and how is it useful?

9) In what scenario decision tree should be preferred over a random forest?

Decision trees are much more interpretable than random forests. A single decision tree can be visualized and understood easily, making it a good choice when you need to explain the logic of the model's decisions to non-technical stakeholders.

Decision tree:
==============

It's a tree-like structure that represents a sequence of decisions and their possible outcomes. Each internal node of the tree represents a decision based on a particular feature, while each leaf node represents a prediction or an outcome.

Random forest:
=============

A Random Forest is a powerful ensemble machine-learning algorithm that combines multiple individual decision trees to create a more accurate and robust model. It's designed to overcome some of the limitations of a single decision tree by leveraging the concept of "wisdom of the crowd."

10) Why Logistic Regression is called regression?

In Logistic regression we do linear regression first we use a Sigmoid Function  to transform the linear combination of features into a value between 0 and 1, representing the probability of the positive class in a binary classification problem.



10) What is Online Machine Learning?

Online machine learning, also known as incremental or streaming machine learning, refers to a machine learning paradigm in which a model is updated continuously as new data becomes available, as opposed to the traditional batch learning approach where the model is trained on a fixed dataset.

Example: ChatBots, Personal Recommendations

11) What is the No Free Lunch Theorem in Machine Learning?

Without taking assumptions it's very difficult to choose a machine learning model for a particular problem. 
